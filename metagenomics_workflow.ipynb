{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41ec5e40-53ac-4ba4-a69d-a0cd03f0760c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv as csv\n",
    "import re\n",
    "\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5348bc1b-8a03-48e3-a845-b03ae10f3bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list paths to software and locations we will need later\n",
    "idba_ud = '/shared/software/bin/idba_ud'\n",
    "mash = '/shared/software/bin/mash'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e499b2-6c3b-48cd-905a-dbbf373cea91",
   "metadata": {},
   "source": [
    "# Define projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef0cbfe2-ccd4-4b4b-bd0e-bdeb7da38224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWTP2:\n",
    "project = 'awtp2'\n",
    "df = pd.read_csv('/Users/rosekantor/data/awtp2/metagenomics/tables/sample_info.csv')\n",
    "df = df.rename(columns={'ggkbase_project_name':'sample_id'})\n",
    "df['combined_reads'] = f'/groups/banfield/sequences/2022/' + df.sample_id + '/raw.d/' + df.sample_id + '_trim_clean.PE.fa'\n",
    "\n",
    "# assign paths\n",
    "assem_dir = f'/groups/banfield/projects/industrial/nelson_lab/awtp2/assemblies'\n",
    "reads_dir = f'/groups/banfield/projects/industrial/nelson_lab/awtp2/reads/trimmed'\n",
    "mash_dir = f'/groups/banfield/projects/industrial/nelson_lab/awtp2/mash_analysis'\n",
    "outdir = f'/Users/rosekantor/data/awtp2/metagenomics/workflow/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12204566-f565-4717-a665-dd3435d26661",
   "metadata": {},
   "source": [
    "# Raw read processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95e20d7a-4164-4f76-88c4-b7e4b2eab77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['name'] = df.sample_id\n",
    "df['contigs_db_path'] = f'{assem_dir}/' + df.sample_id + '.idba_ud/' + df.sample_id + '_contigs.db'\n",
    "df['profile_db_path'] = f'{assem_dir}/' + df.sample_id + '.idba_ud/anvio_data/' + df.sample_id + '_profile/PROFILE.db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4140792-e612-4fe7-9fa8-15f9f66ee065",
   "metadata": {},
   "outputs": [],
   "source": [
    "metagenome_table = df[~df['name'].str.contains('CTRL')][['name', 'contigs_db_path', 'profile_db_path']]\n",
    "metagenome_table.to_csv(f'{outdir}/metagenome_table.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5412f1-63fb-477a-ac6b-fa99bbbbbb5a",
   "metadata": {},
   "source": [
    "1. Run `bbmap` and `sickle` on raw reads.\n",
    "\n",
    "2. Make symlinks to reads in our analysis folders using `ln -s` so that we can have easy access to them in the future.\n",
    "\n",
    "3. Review the output of fastqc to check the quality of the reads.  If reads appear to be of low or questionable quality, run fastqc on the trimmed reads to check that quality of the remaining reads is higher after trimming and any concerning artifacts have been removed.\n",
    "\n",
    "4. Count the reads before and after trimming. From within /trimmed and /raw, open a tmux session and run this command for counting reads:\n",
    "    \n",
    "`seqkit stat *.fastq.gz > seqkit_output.tsv`\n",
    "\n",
    "5. Combine the sample project names table with the output table from seqkit to track the data for each sample.  How much total sequencing per sample, how many forward/reverse reads before/after trimming?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9996a369-4014-403a-b3dd-002244108c88",
   "metadata": {},
   "source": [
    "# Assembly commands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4696be85-879a-4207-95c1-717786daab5a",
   "metadata": {},
   "source": [
    "Using the sample table, generate commands for running assemblies on the cluster (via SLURM). Commands may specify whether to use the memory or high-memory nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "50c5dd34-723f-45bc-b8fb-080dd9fba771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will create an empty list, iterate thru the df and generate commands, save them in the list, then turn the list into a column in our df\n",
    "idba_cmd = []\n",
    "for row in df.itertuples():\n",
    "    # some samples may require more memory for assembly\n",
    "    if 'INF' in row.sample_id or 'BAC' in row.sample_id:\n",
    "        cmd = f'sbatch --partition memory --wrap \"' \\\n",
    "              f'{idba_ud} ' \\\n",
    "              f'--pre_correction -r {row.combined_reads} ' \\\n",
    "              f'-o {assem_dir}/{row.sample_id}.idba_ud\"'\n",
    "    else:\n",
    "        cmd = f'sbatch --wrap \"' \\\n",
    "          f'{idba_ud} ' \\\n",
    "          f'--pre_correction -r {row.combined_reads} ' \\\n",
    "          f'-o {assem_dir}/{row.sample_id}.idba_ud\"'\n",
    "\n",
    "    idba_cmd.append(cmd)\n",
    "df['idba_cmd'] = idba_cmd\n",
    "\n",
    "# write the commands to a CSV\n",
    "df['idba_cmd'].to_csv(f'{outdir}/1.assembly.sh', index=False, quoting=csv.QUOTE_NONE, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdd4724-bdf9-4f92-b329-db9114c5a25f",
   "metadata": {},
   "source": [
    "note: alternatively you could just open a file and write the commands directly to the file as you create them (in the for loop). You could also open a jupyter notebook on biotite and run the commands directly from there (not recommended because it's harder to track what is running).\n",
    "\n",
    "Now push this shell script up to biotite and run it: `sh assembly.sh`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb4d3c5-e336-4970-ab50-c9477a9a5fd2",
   "metadata": {},
   "source": [
    "# Mash commands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b766f307-08cc-4ec1-8271-e97332f54c38",
   "metadata": {},
   "source": [
    "Compare samples to each other using minhash distances between reads files.  First, need to combine forward and reverse reads into a single stream and then create a mash sketch for each sample.  Then all samples can be pairwise compared to each other to produce a distance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "01655242-e1f2-43e8-b186-b1d182d4df65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mash all v. all reads\n",
    "mash_cmd = []\n",
    "for row in df.itertuples():\n",
    "    cmd = f'cat {reads_dir}/{s}_trim_clean.PE.1.fastq.gz {reads_dir}/{s}_trim_clean.PE.2.fastq.gz | {mash} sketch -m 2 -r - -I {row.sample_id} -s 10000 -o {mash_dir}/{row.sample_id}'\n",
    "    mash_cmd.append(cmd)\n",
    "    \n",
    "df['mash_cmd'] = mash_cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "133c9484-5cad-448f-951a-b407a6c6cd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the commands to a CSV\n",
    "df['mash_cmd'].to_csv(f'{outdir}/2.mash.sh', index=False, quoting=csv.QUOTE_NONE, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68281cb-f04a-400f-a8a2-a28f08fa8971",
   "metadata": {},
   "source": [
    "## mash dist commands\n",
    "`mash paste` combines multiple sketches into a single sketch.  The first arg is output name, followed by a list of all the sketch files you want to combine\n",
    "\n",
    "Command: `mash paste awtp2.msh *msh`\n",
    "\n",
    "`mash dist` can sketch on the fly or take a sketch as input.  Because we are doing all-vs-all we use the same msh file as the query and reference.\n",
    "\n",
    "Command: `mash dist awtp2.msh awtp2.msh`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7769096-6c90-4935-bffd-920a9d60b243",
   "metadata": {},
   "source": [
    "# Post-assembly commands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40ae0a6-f6c5-417c-8caf-19d926924ede",
   "metadata": {},
   "source": [
    "1. Rename the scaffolds so that they have the project name in the fasta headers.\n",
    "2. Check assembly quality using Itai Sharon's contig_stats.pl \n",
    "3. Filter scaffolds > 1000 bp into a new min1000.fa file for further analysis.\n",
    "4. Remove extra files created by the assembler\n",
    "5. Make bowtie2 indexes in their own folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "3942f2a3-f5cb-42e8-a36e-d1477ef0ae58",
   "metadata": {},
   "outputs": [],
   "source": [
    "postassem_cmd = [] # create empty list\n",
    "for row in df.itertuples():\n",
    "    s = row.sample_id\n",
    "    \n",
    "    scaffolds = f'{assem_dir}/{s}.idba_ud/{s}_scaffold.fa'\n",
    "\n",
    "    # include \"sample_id\" in headers and rename file to sample_id_scaffold.fa\n",
    "    rehead = f\"sed 's/scaffold/{s}/g' {assem_dir}/{s}.idba_ud/scaffold.fa > {scaffolds}\"\n",
    "\n",
    "    # get contig stats\n",
    "    cstats = f'contig_stats.pl -i {scaffolds}'\n",
    "\n",
    "    # delete extra files from assembly\n",
    "\n",
    "    clean = f'rm {assem_dir}/{s}.idba_ud/kmer '\\\n",
    "            f'{assem_dir}/{s}.idba_ud/contig-* '\\\n",
    "            f'{assem_dir}/{s}.idba_ud/align-* '\\\n",
    "            f'{assem_dir}/{s}.idba_ud/graph-* '\\\n",
    "            f'{assem_dir}/{s}.idba_ud/local-*'\n",
    "\n",
    "    # make directory to store bowtie2 indices in\n",
    "    mdbt2 = f'mkdir {assem_dir}/{s}.idba_ud/bt2/'\n",
    "\n",
    "    # index in prep for bowtie2 mapping to get true coverage for ggkbase\n",
    "    ind = f'bowtie2-build {scaffolds} {assem_dir}/{s}.idba_ud/bt2/{s}_scaffold.fa'\n",
    "    \n",
    "    cmd = [rehead, cstats, clean, mdbt2, ind]\n",
    "    all_cmd = '; '.join(cmd) # separate all commands by semicolon (so they will be executed in order for each sample)\n",
    "    postassem_cmd.append(all_cmd) # append command to list\n",
    "    \n",
    "df['postassem_cmd'] = postassem_cmd # add as a column to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "34d3680f-996e-4f16-b00e-dee59fc6b68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the commands to a CSV\n",
    "df['postassem_cmd'].to_csv(f'{outdir}/3.postassem.sh', index=False, quoting=csv.QUOTE_NONE, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf9e1a6-00d8-4e96-a5d6-ff7ea6cb1586",
   "metadata": {},
   "source": [
    "# ggkbase import: \n",
    "## Mapping and gene-calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c4c0a399-1a3f-40c0-9dc6-38154f24ea3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "shrinksam = '/shared/software/bin/shrinksam'\n",
    "bt2 = '/shared/software/bin/bowtie2'\n",
    "\n",
    "self_map_cmd = []\n",
    "readcounts_cmd = []\n",
    "genecalls_cmd = []\n",
    "\n",
    "for row in df.itertuples():\n",
    "    \n",
    "    s = row.sample_id\n",
    "    bt2base = f'{assem_dir}/{s}.idba_ud/bt2/{s}_scaffold.fa'\n",
    "    r1 = f'{reads_dir}/{s}_trim_clean.PE.1.fastq.gz'\n",
    "    r2 = f'{reads_dir}/{s}_trim_clean.PE.2.fastq.gz'\n",
    "    scaffolds = f'{assem_dir}/{s}.idba_ud/{s}_scaffold.fa'\n",
    "    scaffolds_min1000 = f'{assem_dir}/{s}.idba_ud/{s}_scaffold_min1000.fa'\n",
    "\n",
    "    # perform self-mapping and add readcounts to scaffold headers\n",
    "    map_cmd = f'sbatch --wrap \"{bt2} -p 48 -x {bt2base} -1 {r1} -2 {r2} 2> {assem_dir}/{s}.idba_ud/{s}_scaffold_mapped.log | {shrinksam} -v > {assem_dir}/{s}.idba_ud/{s}_scaffold_mapped.sam\"'\n",
    "    self_map_cmd.append(map_cmd)\n",
    "    \n",
    "    add_readcount = f'/groups/banfield/software/pipeline/v1.1/scripts/add_read_count.rb {assem_dir}/{s}.idba_ud/{s}_scaffold_mapped.sam {assem_dir}/{s}.idba_ud/{s}_scaffold.fa 150 > {assem_dir}/{s}.idba_ud/{s}_scaffold.fa.counted'\n",
    "    move_file = f'mv {assem_dir}/{s}.idba_ud/{s}_scaffold.fa.counted {assem_dir}/{s}.idba_ud/{s}_scaffold.fa'\n",
    "    \n",
    "    # filter for only contigs ≥1000 bp\n",
    "    min1000 = f'pullseq -i {scaffolds} --min 1000 > {scaffolds_min1000}'\n",
    "    \n",
    "    readcounts_cmd.append(f'{add_readcount}; {move_file}; {min1000}') # append command to list\n",
    "    \n",
    "    # gene-calling\n",
    "    call_orfs = f'prodigal -i {scaffolds_min1000} -o {scaffolds_min1000}.genes -a {scaffolds_min1000}.genes.faa -d {scaffolds_min1000}.genes.fna -m -p meta'\n",
    "    call_16S = f'/groups/banfield/software/pipeline/v1.1/scripts/16s.sh {scaffolds_min1000} > {scaffolds_min1000}.16s'\n",
    "    call_trnas = f'/groups/banfield/software/pipeline/v1.1/scripts/trnascan_pusher.rb -i {scaffolds_min1000} > /dev/null 2>&1'\n",
    "    genecalls_cmd.append(f'{call_orfs}; {call_16S}; {call_trnas}')\n",
    "\n",
    "df['self_map_cmd'] = self_map_cmd\n",
    "df['readcounts_cmd'] = readcounts_cmd\n",
    "df['genecalls_cmd'] = genecalls_cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "568d4dd6-4a37-4252-8599-11cd34e2e580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the commands to a CSV\n",
    "df['self_map_cmd'].to_csv(f'{outdir}/4.ggkbase_self_map.sh', index=False, quoting=csv.QUOTE_NONE, header=False)\n",
    "df['readcounts_cmd'].to_csv(f'{outdir}/5.ggkbase_readcounts.sh', index=False, quoting=csv.QUOTE_NONE, header=False)\n",
    "df['genecalls_cmd'].to_csv(f'{outdir}/6.ggkbase_genecalls.sh', index=False, quoting=csv.QUOTE_NONE, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e20949e-2512-47ef-853c-46575e1c5f8d",
   "metadata": {},
   "source": [
    "## Annotation by best reciprocal blast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "91fab103-b92f-4aaf-ace2-3a07fdaca51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "usearch = '/groups/banfield/software/pipeline/v1.1/scripts/cluster_usearch_wrev.rb'\n",
    "annolookup = '/shared/software/bin/annolookup.py'\n",
    "\n",
    "usearch_cmds = []\n",
    "anno_proccess_cmds = []\n",
    "\n",
    "for row in df.itertuples():\n",
    "    s = row.sample_id\n",
    "    faa = f'{assem_dir}/{s}.idba_ud/{s}_scaffold_min1000.fa.genes.faa'\n",
    "    \n",
    "    # make usearch commands (using cluster usearch wrapper)\n",
    "    search_kegg = f'sbatch --wrap \"{usearch} -i {faa} -k -d kegg --nocluster\"'\n",
    "    search_uni = f'sbatch --wrap \"{usearch} -i {faa} -k -d uni --nocluster\"'\n",
    "    search_uniprot = f'sbatch --wrap \"{usearch} -i {faa} -k -d uniprot --nocluster\"'\n",
    "    \n",
    "    usearch_cmds.append(f'{search_kegg}; {search_uni}; {search_uniprot}')\n",
    "\n",
    "    # gzip files\n",
    "    gzip_b6 = f'gzip {faa}*.b6'\n",
    "    \n",
    "    # annolookup\n",
    "    anno_kegg = f'{annolookup} {faa}-vs-kegg.b6.gz kegg > {faa}-vs-kegg.b6+'\n",
    "    anno_uni = f'{annolookup} {faa}-vs-uni.b6.gz uniref > {faa}-vs-uni.b6+'\n",
    "    anno_uniprot = f'{annolookup} {faa}-vs-uniprot.b6.gz uniprot > {faa}-vs-uniprot.b6+'\n",
    "    \n",
    "    anno_proccess_cmds.append(f'{gzip_b6}; {anno_kegg}; {anno_uni}; {anno_uniprot}')\n",
    "\n",
    "df['usearch_cmds'] = usearch_cmds\n",
    "df['anno_proccess_cmds'] = anno_proccess_cmds    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b3a5583d-2aaf-4dd2-a58e-299b3d9e576f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the commands to a CSV\n",
    "df['usearch_cmds'].to_csv(f'{outdir}/7.ggkbase_usearch.sh', index=False, quoting=csv.QUOTE_NONE, header=False)\n",
    "df['anno_proccess_cmds'].to_csv(f'{outdir}/8.ggkbase_anno_process.sh', index=False, quoting=csv.QUOTE_NONE, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07675f8a-6a66-4376-9419-4c0591c6fa8c",
   "metadata": {},
   "source": [
    "The files have now been generated for ggkbase import."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ed043f-f3b6-47bb-8e1d-b941780ead24",
   "metadata": {},
   "source": [
    "# Anvi'o process contigs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c8b233-f9f6-4083-9826-e14ec372403b",
   "metadata": {},
   "source": [
    "See https://merenlab.org/2016/06/18/importing-taxonomy/ and https://merenlab.org/2016/06/22/anvio-tutorial-v2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8c213b4d-e4c4-464d-bd8d-f16ec5836be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "awk = \"awk '{print $1}'\" # using this to remove additional fields from fasta headers for contigs, ignore if you don't have this issue\n",
    "anvio = 'conda activate anvio-7.1' # note that this didn't actually work, needed to use conda run but we used system anvio v7\n",
    "\n",
    "kaiju_path = '/groups/banfield/projects/industrial/nelson_lab/kaiju/bin'\n",
    "kaiju_nodes = '/shared/db/kaiju/nr_euk/r2021-02-24/nodes.dmp'\n",
    "kaiju_names = '/shared/db/kaiju/nr_euk/r2021-02-24/names.dmp'\n",
    "\n",
    "kaiju_nr = '/shared/db/kaiju/nr_euk/r2021-02-24/kaiju_db_nr_euk.fmi'\n",
    "kaiju = f'{kaiju_path}/kaiju -t {kaiju_nodes} -f {kaiju_nr}'\n",
    "kaiju_addtaxnames = f'{kaiju_path}/kaiju-addTaxonNames -t {kaiju_nodes} -n {kaiju_names} -r superkingdom,phylum,order,class,family,genus,species'\n",
    "\n",
    "fix_scaffolds_cmd = []\n",
    "analyze_contigs_cmd = []\n",
    "addTaxonomy_cmd = []\n",
    "\n",
    "for row in df.itertuples():\n",
    "    s = row.sample_id\n",
    "    \n",
    "    # define file names\n",
    "    scaffolds_badheaders = f'{assem_dir}/{s}.idba_ud/{s}_scaffold_min1000.fa'\n",
    "    scaffolds_min1000 = f'{assem_dir}/{s}.idba_ud/{s}_min1000.fa'\n",
    "    contigsDB = f'{assem_dir}/{s}.idba_ud/{s}_contigs.db'\n",
    "    gene_calls = f'{assem_dir}/{s}.idba_ud/{s}_gene_calls.fa'\n",
    "    kaiju_out = f'{assem_dir}/{s}.idba_ud/{s}_kaiju.out'\n",
    "    kaiju_processed = f'{assem_dir}/{s}.idba_ud/{s}_genes_kaiju.txt'\n",
    "    \n",
    "    # fix headers\n",
    "    cmd = f'{awk} {scaffolds_badheaders} > {scaffolds_min1000}'\n",
    "    fix_scaffolds_cmd.append(cmd)\n",
    "    \n",
    "    # write commands for contigs analysis on cluster\n",
    "    make_cdb = f'anvi-gen-contigs-database -f {scaffolds_min1000} -o {contigsDB} -n {s} -T 48'\n",
    "    get_genes = f'anvi-get-sequences-for-gene-calls -c {contigsDB} -o {gene_calls}'\n",
    "    anvhmms = f'anvi-run-hmms -c {contigsDB} -T 48'\n",
    "    anvscg = f'anvi-run-scg-taxonomy -c {contigsDB} -T 48' # this didn't actually run because it wasn't set up yet!\n",
    "    #anvcogs = f'/shared/software/bin/anvi-run-ncbi-cogs -c {contigsDB} -T 48'\n",
    "    run_kaiju = f'{kaiju} -i {gene_calls} -v -z 48 > {kaiju_out}' \n",
    "    cmd = f'sbatch --wrap \"{anvio}; {make_cdb}; {get_genes}; {anvhmms}; {anvscg}; {run_kaiju}\"'\n",
    "    analyze_contigs_cmd.append(cmd)\n",
    "    \n",
    "    ## write commands for local jobs after cluster jobs\n",
    "    process_kaiju = f'{kaiju_addtaxnames} -i {kaiju_out} -o {kaiju_processed}' \n",
    "    import_kaiju = f'anvi-import-taxonomy-for-genes -i {kaiju_processed} -c {contigsDB} -p kaiju --just-do-it'\n",
    "    cmd = f'{process_kaiju}; {import_kaiju}'\n",
    "    addTaxonomy_cmd.append(cmd)\n",
    "\n",
    "# add to dataframe\n",
    "df['fix_scaffolds_cmd'] = fix_scaffolds_cmd\n",
    "df['analyze_contigs_cmd'] = analyze_contigs_cmd\n",
    "df['addTaxonomy_cmd'] = addTaxonomy_cmd\n",
    "\n",
    "# save files\n",
    "df['fix_scaffolds_cmd'].to_csv(f'{outdir}/anvio_0-fix_scaffold_names.sh', index=False, quoting=csv.QUOTE_NONE, header=False)\n",
    "df['analyze_contigs_cmd'].to_csv(f'{outdir}/anvio_1-analyze_contigs.sh', index=False, quoting=csv.QUOTE_NONE, header=False)\n",
    "df['addTaxonomy_cmd'].to_csv(f'{outdir}/anvio_2-addTaxonomy.sh', index=False, quoting=csv.QUOTE_NONE, header=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa5c8c5-34d5-4a13-b9c6-009cd5a7c4a2",
   "metadata": {},
   "source": [
    "you must run `conda activate anvio-7.1` in terminal before running the addTaxonomy commands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101aaf47-5598-4031-8dc6-09fcada7136b",
   "metadata": {},
   "source": [
    "# SCG taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a271c10-c49e-48da-85cd-d371660460f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>estscg_taxonomy_cmd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ARSTAG_AR_4_27</td>\n",
       "      <td>anvi-estimate-scg-taxonomy -c /groups/banfield...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ARSTAG_ARBF_12345_pre</td>\n",
       "      <td>anvi-estimate-scg-taxonomy -c /groups/banfield...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ARSTAG_ARBF_1_post</td>\n",
       "      <td>anvi-estimate-scg-taxonomy -c /groups/banfield...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ARSTAG_ARBF_2_post</td>\n",
       "      <td>anvi-estimate-scg-taxonomy -c /groups/banfield...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ARSTAG_ARBF_3_post</td>\n",
       "      <td>anvi-estimate-scg-taxonomy -c /groups/banfield...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               sample_id                                estscg_taxonomy_cmd\n",
       "0         ARSTAG_AR_4_27  anvi-estimate-scg-taxonomy -c /groups/banfield...\n",
       "1  ARSTAG_ARBF_12345_pre  anvi-estimate-scg-taxonomy -c /groups/banfield...\n",
       "2     ARSTAG_ARBF_1_post  anvi-estimate-scg-taxonomy -c /groups/banfield...\n",
       "3     ARSTAG_ARBF_2_post  anvi-estimate-scg-taxonomy -c /groups/banfield...\n",
       "4     ARSTAG_ARBF_3_post  anvi-estimate-scg-taxonomy -c /groups/banfield..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f81c92fd-8fd4-423a-b093-4c0352ad1d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run-scg failed above because set-up-scg hadn't been run so rerunning\n",
    "\n",
    "scg_taxonomy_cmd = []\n",
    "\n",
    "for row in df.itertuples():\n",
    "    s = row.sample_id\n",
    "    \n",
    "    contigsDB = f'{assem_dir}/{s}.idba_ud/{s}_contigs.db'\n",
    "\n",
    "    anvscg = f'anvi-run-scg-taxonomy -c {contigsDB} -T 16' # this didn't actually run because it wasn't set up yet!\n",
    "    scg_taxonomy_cmd.append(anvscg)\n",
    "\n",
    "df['scg_taxonomy_cmd'] = scg_taxonomy_cmd\n",
    "\n",
    "# save files\n",
    "df['scg_taxonomy_cmd'].to_csv(f'{outdir}/anvio_scg_taxonomy_cmd.sh', index=False, quoting=csv.QUOTE_NONE, header=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0c0a861-df63-42c3-b12f-20a19a53039c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate SCG taxonomy\n",
    "# must migrate contigs.dbs to 7.1 and run this with 7.1\n",
    "estscg_taxonomy_cmd = []\n",
    "\n",
    "for row in df.itertuples():\n",
    "    s = row.sample_id\n",
    "    \n",
    "    contigsDB = f'{assem_dir}/{s}.idba_ud/{s}_contigs.db'\n",
    "    profileDB = f'{assem_dir}/{s}.idba_ud/anvio_data/merged/PROFILE.db'\n",
    "    out = f'{assem_dir}/{s}.idba_ud/anvio_data/{s}_scg_taxonomy.txt'\n",
    "    anvestscg = f'anvi-estimate-scg-taxonomy -c {contigsDB} -T 16 --metagenome-mode -o {out} -p {profileDB} --compute-scg-coverages -p {profileDB} --compute-scg-coverages -S Ribosomal_S2'\n",
    "    estscg_taxonomy_cmd.append(anvestscg)\n",
    "\n",
    "df['estscg_taxonomy_cmd'] = estscg_taxonomy_cmd\n",
    "\n",
    "# save files\n",
    "df['estscg_taxonomy_cmd'].to_csv(f'{outdir}/anvio_estscg_taxonomy_cov_cmd.sh', index=False, quoting=csv.QUOTE_NONE, header=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35dbd24-8c11-4099-b6ba-ed6867ff3f57",
   "metadata": {},
   "source": [
    "# Cross-mapping reads and anvio profile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5139ab4-9260-4f57-abde-847888cdd113",
   "metadata": {},
   "source": [
    "Build bt2 directories for our min1000 scaffolds to match the Anvio contigs db scaffolds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8e05a3f3-6d91-4a12-b156-90042adb24a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{outdir}/anvio_3-bt2build.sh', 'w') as f:\n",
    "    for row in df.itertuples():\n",
    "        s = row.sample_id\n",
    "        scaffolds = f'{assem_dir}/{s}.idba_ud/{s}_min1000.fa'\n",
    "        bt2ind = f'{assem_dir}/{s}.idba_ud/bt2/{s}_min1000.fa'\n",
    "        # index in prep for bowtie2 mapping to get coverage on min1000 scaffolds\n",
    "        f.write(f'bowtie2-build {scaffolds} {bt2ind}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "59f9f16d-a121-44cf-9a40-7535bb853aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "shrinksam = '/shared/software/bin/shrinksam'\n",
    "bt2 = '/shared/software/bin/bowtie2'\n",
    "anvio = 'conda run -n anvio-7.1'\n",
    "\n",
    "xmapping_df = []\n",
    "for srow in df.itertuples():  \n",
    "    s = srow.sample_id\n",
    "\n",
    "    scaffolds = f'{assem_dir}/{s}.idba_ud/{s}_min1000.fa'\n",
    "    bt2base = f'{assem_dir}/{s}.idba_ud/bt2/{s}_min1000.fa'\n",
    "    contigsDB = f'{assem_dir}/{s}.idba_ud/{s}_contigs.db'\n",
    "    \n",
    "    # map against other samples and controls\n",
    "    for rrow in df.itertuples():\n",
    "        r = rrow.sample_id\n",
    "        r_fixed = re.sub('[\\.-]', '_', r) # if your sample name has dashes or periods in it, Anvio won't like it.\n",
    "        \n",
    "        r1 = f'{reads_dir}/{r}_trim_clean.PE.1.fastq.gz'\n",
    "        r2 = f'{reads_dir}/{r}_trim_clean.PE.2.fastq.gz'        \n",
    "        raw_bam = f'{assem_dir}/{s}.idba_ud/{s}-vs-{r}-raw.bam'\n",
    "        filtered_bam_raw = f'{assem_dir}/{s}.idba_ud/{s}-vs-{r}-raw-filt.bam'\n",
    "        bam = f'{assem_dir}/{s}.idba_ud/{s}-vs-{r}.bam'\n",
    "        \n",
    "        if 'CTRL' in r or 'CONTROL' in r:\n",
    "            \n",
    "            # filter mapping - for decontamination, we want to use filtered mappings\n",
    "            map_cmd = f'sbatch --wrap \"{bt2} -p 48 -x {bt2base} -1 {r1} -2 {r2} | {shrinksam} -v | sambam > {raw_bam}; ' \\\n",
    "                      f'reformat.sh in={raw_bam} out={filtered_bam_raw} editfilter=2 threads=48; rm {raw_bam}\"'\n",
    "        \n",
    "        else:\n",
    "            map_cmd = f'sbatch --wrap \"{bt2} -p 48 -x {bt2base} -1 {r1} -2 {r2} | {shrinksam} -v | sambam > {filtered_bam_raw}\"'\n",
    "        \n",
    "        # map_cmd = f'sbatch --wrap \"{bt2} -p 48 -x {bt2base} -1 {r1} -2 {r2} | {shrinksam} -v | sambam > {filtered_bam_raw}\"'\n",
    "        \n",
    "        # process mapping\n",
    "        initbam = f'anvi-init-bam {filtered_bam_raw} -o {bam}; rm {filtered_bam_raw}'\n",
    "        \n",
    "        # anvi-profile\n",
    "        profile_out = f'{assem_dir}/{s}.idba_ud/anvio_data/{r}_profile' # check this\n",
    "        anvip_cmd = f'sbatch --wrap \"{anvio} anvi-profile --min-contig-length 1000 --skip-SNV-profiling -T 48 -i {bam} -c {contigsDB} -o {profile_out} -S {r_fixed}\"'\n",
    "        \n",
    "        ## append all commands to table\n",
    "        xmapping_df.append([s, r, map_cmd, initbam, anvip_cmd])\n",
    "xmapping_df = pd.DataFrame.from_records(xmapping_df, columns=['assembly', 'reads', 'map', 'initbam', 'anvi-profile'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "76e19f8d-c2e4-4e25-9332-79150c4a57cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "xmapping_df.to_csv(f'/Users/rosekantor/data/metagenomics_mentoring/{project}/crossmapping.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1a7e65b0-c157-4920-9d75-796d539ebb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of filtering the xmapping_df to get just the rows you want\n",
    "pos = xmapping_df[(xmapping_df.assembly == 'AWTP2_CTRL_POS-POWER9_EXTRACTION_0') \n",
    "                  & ((xmapping_df.reads.str.contains('POS')) | (xmapping_df.reads.str.contains('BLANK')))]\n",
    "\n",
    "pos['map'].to_csv(f'{outdir}/anvio_xmap_pos-controls.sh', index=False, quoting=csv.QUOTE_NONE, header=False)\n",
    "pos['initbam'].to_csv(f'{outdir}/anvio_initbam_pos-controls.sh', index=False, quoting=csv.QUOTE_NONE, header=False)\n",
    "pos['anvi-profile'].to_csv(f'{outdir}/anvio_anvip_pos-controls.sh', index=False, quoting=csv.QUOTE_NONE, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae829929-53fe-43f9-a7f3-cebfa4b0b673",
   "metadata": {},
   "source": [
    "Based on MASH distances between samples, decide which samples to cross-map against one another. A rule of thumb is that cross-mapping beyond 12 samples likely will not improve your bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3021bebb-f19c-4a7a-83b9-57fcb007ecea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-18-685a643f1953>:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df1['anvi-profile'] = df1['anvi-profile'].str.replace('conda run -n anvio-7.1 ', '')\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv(f'{outdir}/rerun_profile.txt', sep=',', names=['assembly','reads','map','initbam','anvi-profile'])\n",
    "df1['anvi-profile'] = df1['anvi-profile'].str.replace('conda run -n anvio-7.1 ', '')\n",
    "df1['anvi-profile'].to_csv(f'{outdir}/rerun_profile.sh', index=False, quoting=csv.QUOTE_NONE, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825a44d3-7b7d-4a18-ae1d-4540fa462b95",
   "metadata": {},
   "source": [
    "# Anvi-merge to combine crossmappings into a single profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97f738f-3e80-4851-bd2b-796624b64391",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{outdir}/anvio_merge.sh', 'w') as f:\n",
    "    for row in df.itertuples():\n",
    "        s = row.sample_id\n",
    "        contigsDB = f'{assem_dir}/{s}.idba_ud/{s}_contigs.db'\n",
    "        profiles = f'{assem_dir}/{s}.idba_ud/anvio_data/*_profile/PROFILE.db'\n",
    "        out_profileDB = f'{assem_dir}/{s}.idba_ud/anvio_data/merged'\n",
    "        merge_cmd = f'anvi-merge -c {contigsDB} -p {profiles} -o {out_profileDB} -S {s}' #  --skip-hierarchical-clustering # --enforce-hierarchical-clustering\n",
    "        f.write(f'{merge_cmd}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201f03f1-f869-4b86-a6c8-1f1f49939102",
   "metadata": {},
   "source": [
    "# Binning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcbff81-63f8-48f1-b3be-fce5ee71b18f",
   "metadata": {},
   "source": [
    "Note: you may want to tell concoct how many bins to create based on the counts of the SCGs in your samples.\n",
    "\n",
    "To do this, you could add a column to your df with that information and then uncomment the lines below (and include in the concoct command)\n",
    "\n",
    "One of the Anvi'o tutorials suggests it's better to underestimate the number of genomes since it is easier to split contaminated bins with anvi-refine than to combine fragmented bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e257b9e-5526-4460-b26a-265e5200d7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = f'{outdir}/anvio_concoct.sh'\n",
    "with open(outfile, 'w') as f:\n",
    "    for row in df.itertuples():\n",
    "        s = row.sample_id\n",
    "        # c = row.scg_count\n",
    "        profileDB = f'{assem_dir}/{s}.idba_ud/anvio_data/merged/PROFILE.db'\n",
    "        contigsDB = f'{assem_dir}/{s}.idba_ud/{s}_contigs.db'\n",
    "        out = f'{assem_dir}/{s}.idba_ud/anvio_data'\n",
    "\n",
    "        get_cococt_input = f'anvi-export-splits-and-coverages -p {profileDB} -c {contigsDB} -o {out} -O {s} --splits-mode'\n",
    "        run_concoct = f'concoct --coverage_file {out}/{s}-COVs.txt --composition_file {out}/{s}-SPLITS.fa -b {out}/{s}_concoct -r 150 -t 10' #-c {c}\n",
    "        csv_to_tsv = f'sed \"s/,/\\\\tbin_/g\" {out}/{s}_concoct_clustering_gt1000.csv > {out}/{s}_concoct_clustering_gt1000.tsv'\n",
    "        import_collection = f'anvi-import-collection {s}_concoct_clustering_gt1000.tsv -p {profileDB} -c {contigsDB} -C concoct_noest'\n",
    "\n",
    "        f.write(f'{get_cococt_input}; {run_concoct}; {csv_to_tsv}; {import_collection}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7d3611-dd21-426f-8f45-1a580bb52e41",
   "metadata": {},
   "source": [
    "Here are the parameters Anvio is using for metabat (based on the log file it created)\n",
    "\n",
    "metabat2 -i /tmp/tmpcs7pncgj/sequence_contigs.fa -a /tmp/tmpcs7pncgj/contig_coverages.txt -o /tmp/tmpcs7pncgj/METABAT_ --cvExt -l\n",
    "\n",
    "MetaBAT 2 (v2.12.1) using minContig 2500, minCV 1.0, minCVSum 1.0, maxP 95%, minS 60, and maxEdges 200\n",
    "\n",
    "Note: it looks like metabat can also take into account the coverage variances for each contig in each sample, which might improve binning.  \n",
    "Anvi'o doesn't send that input to metabat, so we would have to separately profile all the bam files with jgi_summarize_bam_contig_depths and feed it to metabat ourselves.  If the bins aren't good enough without this, then we can try it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e4dc30e9-9dd0-4b36-8ebe-94948d5375f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "anvio = 'conda run -n anvio-7.1'\n",
    "outfile = f'{outdir}/anvio_metabat.sh'\n",
    "with open(outfile, 'w') as f:\n",
    "    for row in df.itertuples():\n",
    "        s = row.sample_id\n",
    "        # c = row.scg_count\n",
    "        profileDB = f'{assem_dir}/{s}.idba_ud/anvio_data/merged/PROFILE.db'\n",
    "        contigsDB = f'{assem_dir}/{s}.idba_ud/{s}_contigs.db'\n",
    "        f.write(f'sbatch --wrap \"{anvio} anvi-cluster-contigs -c {contigsDB} -p {profileDB} --driver metabat2 -C metabat -T 48 --just-do-it\"\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad6402a9-a0b7-40aa-a1d8-95aace8bde59",
   "metadata": {},
   "outputs": [],
   "source": [
    "anvio = '~/github/anvio/bin'\n",
    "outfile = f'{outdir}/anvio_metabat_split.sh'\n",
    "with open(outfile, 'w') as f:\n",
    "    for row in df.itertuples():\n",
    "        s = row.sample_id\n",
    "        # c = row.scg_count\n",
    "        profileDB = f'{assem_dir}/{s}.idba_ud/split/none/PROFILE.db'\n",
    "        contigsDB = f'{assem_dir}/{s}.idba_ud/split/none/CONTIGS.db'\n",
    "        f.write(f'sbatch --wrap \"{anvio}/anvi-cluster-contigs -c {contigsDB} -p {profileDB} --driver metabat2 -C metabat -T 48 --just-do-it\"\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae71b222-056a-42a7-b581-d7f0276ae5f6",
   "metadata": {},
   "source": [
    "## running snakemake binning pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87cdc548-fadc-431b-8107-ce593f5404bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract contigs\n",
    "outfile = f'{outdir}/anvio_export_decontam_contigs.sh'\n",
    "with open(outfile, 'w') as f:\n",
    "    for row in df.itertuples():\n",
    "        s = row.sample_id\n",
    "        if 'CTRL' in s:\n",
    "            continue\n",
    "        contigsDB = f'{assem_dir}/{s}.idba_ud/split/none/CONTIGS.db'\n",
    "        fasta = f'{assem_dir}/{s}.idba_ud/split/none/{s}_contigs.fa'\n",
    "        f.write(f'anvi-export-contigs -c {contigsDB} -o {fasta}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8bfb8654-e2a5-4ed3-9cca-9eaf19955eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make samples.tsv config file for snakemake run\n",
    "\n",
    "df_snakemake = df[['sample_id']].copy().sort_values('sample_id')\n",
    "df_snakemake = df_snakemake.rename(columns={'sample_id': 'sample_name'})\n",
    "\n",
    "\n",
    "df_snakemake['assembly_path'] = df_snakemake.apply(lambda x: f'{assem_dir}/{x.sample_name}.idba_ud/split/none/{x.sample_name}_contigs.fa', axis=1)\n",
    "df_snakemake['forward_read_path'] = df_snakemake.apply(lambda x: f'{reads_dir}/{x.sample_name}_trim_clean.PE.1.fastq.gz', axis=1) \n",
    "df_snakemake['reverse_read_path'] = df_snakemake.apply(lambda x: f'{reads_dir}/{x.sample_name}_trim_clean.PE.2.fastq.gz', axis=1)\n",
    "\n",
    "# just MF_RO\n",
    "df_mfro = df_snakemake[df_snakemake.sample_name.str.contains('MF') | \n",
    "                        df_snakemake.sample_name.str.contains('RO2_BIOFILM')].copy()\n",
    "df_mfro = df_mfro[~df_mfro.sample_name.str.contains('CTRL')]\n",
    "df_mfro.to_csv(f'{outdir}/binning_snakemake_mfro_samples.tsv', sep='\\t', index=False)\n",
    "\n",
    "# # just RO\n",
    "# df_ro = df_snakemake[df_snakemake.sample_name.str.contains('RO2_BULK')].copy()\n",
    "# df_ro = df_ro[~df_ro.sample_name.str.contains('CTRL')]\n",
    "# df_ro.to_csv(f'{outdir}/binning_snakemake_ro_samples.tsv', sep='\\t', index=False)\n",
    "\n",
    "# snakemake -j10 --use-conda --cluster \"sbatch -J ggbin\" --cluster-cancel scancel --latency-wait 60 --rerun-incomplete --dry-run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7791c56d-9ed7-4cb0-8304-a89efccc0c25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/groups/banfield/projects/industrial/nelson_lab/awtp2/assemblies/AWTP2_CTRL_BLANK_EXTRACTION_0.idba_ud/split/none/CONTIGS.db'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_snakemake.assembly_path.values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea56880-464c-494b-85df-3f0f2edaef84",
   "metadata": {},
   "source": [
    "## Check bin quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a0a610f-20d1-480a-bdac-802fafd805bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins_dir = '/groups/banfield/projects/industrial/nelson_lab/awtp2/workflow/ggBin/results'\n",
    "samples = ['AWTP2_SAMPLE_RO2_BULK_3', 'AWTP2_SAMPLE_RO2_BULK_4', 'AWTP2_SAMPLE_RO2_BULK_5', 'AWTP2_SAMPLE_RO2_BULK_6']\n",
    "\n",
    "outfile = f'{outdir}/checkm2_dastool_ggBin.sh'\n",
    "with open(outfile, 'w') as f:\n",
    "    for s in samples:\n",
    "        bins = f'{bins_dir}/{s}/bins/dastool/results/{s}.asm.fa.dastool/bins/'\n",
    "        output = f'{bins_dir}/{s}/bins/dastool/results/{s}.asm.fa.dastool/checkm2'\n",
    "        f.write(f'sbatch --wrap \"checkm2 predict --input {bins} --output-directory {output} --threads 48 -x .fa\"\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb370bb-7256-4ff1-b454-78abbf393b72",
   "metadata": {},
   "source": [
    "# Annotation with KEGG KOFams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2aa277fd-ee41-400c-9a63-5883f6d3456b",
   "metadata": {},
   "outputs": [],
   "source": [
    "anvio = 'conda run -n anvio-7.1'\n",
    "outfile = f'{outdir}/anvio_run_kegg.sh'\n",
    "with open(outfile, 'w') as f:\n",
    "    for row in df.itertuples():\n",
    "        s = row.sample_id\n",
    "        contigsDB = f'{assem_dir}/{s}.idba_ud/{s}_contigs.db'\n",
    "        f.write(f'sbatch --wrap \"{anvio} anvi-run-kegg-kofams -c {contigsDB} -T 48\"\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e673d35-f0a1-4029-8031-effbe7da18e2",
   "metadata": {},
   "source": [
    "# For AWTP2 decontaminated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11eb0dcf-3a4e-4aed-9a93-7094e46ea366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for some reason the RNA HMMs were not run on these dbs\n",
    "anvio = 'conda run -n anvio-7.1'\n",
    "anvihmms_rerun = []\n",
    "\n",
    "for row in df.itertuples():\n",
    "    s = row.sample_id\n",
    "    contigsDB = f'{assem_dir}/{s}.idba_ud/split/none/CONTIGS.db'\n",
    "    anvhmms = f'anvi-run-hmms -c {contigsDB} -T 48 -I Ribosomal_RNA_18S,Ribosomal_RNA_28S,Ribosomal_RNA_16S,Ribosomal_RNA_5S,Ribosomal_RNA_23S --just-do-it'\n",
    "    cmd = f'sbatch --wrap \"{anvio} {anvhmms}\"'\n",
    "    \n",
    "    anvihmms_rerun.append(cmd)\n",
    "    \n",
    "# add to dataframe\n",
    "\n",
    "df['anvihmms_rerun'] = anvihmms_rerun\n",
    "\n",
    "# save files\n",
    "df['anvihmms_rerun'].to_csv(f'{outdir}/anvio_hmms_rerun.sh', index=False, quoting=csv.QUOTE_NONE, sep='\\t', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b5525f51-5220-407a-856f-784945ef4334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add SCG count per metagenome to table to provide CONCOCT with an estimate number of bins to create\n",
    "scg_counts = []\n",
    "files = glob('/Users/rosekantor/data/metagenomics_mentoring/awtp2/phylogenetic_analyses/scg_taxonomy/*.txt')\n",
    "for f in files:\n",
    "    sample_id = re.match(r'.+\\/(.+)_scg_taxonomy.txt', f).group(1)\n",
    "    df_scg = pd.read_csv(f, sep='\\t')\n",
    "    scg_count = (len(df_scg))\n",
    "    scg_counts.append([sample_id, scg_count])\n",
    "\n",
    "scg_counts = pd.DataFrame.from_records(scg_counts, columns=('sample_id', 'scg_count'))\n",
    "df = df.merge(scg_counts, how='left', on='sample_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9e7f287f-a898-40dd-9db1-923efa373026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run with anvio-dev version\n",
    "# split up commands so that concoct can be run on cluster to avoid memory issues and using all the threads\n",
    "\n",
    "concoct_input = []\n",
    "run_concoct = []\n",
    "import_concoct = []\n",
    "\n",
    "for row in df.itertuples():\n",
    "    s = row.sample_id\n",
    "    c = row.scg_count\n",
    "    profileDB = f'{assem_dir}/{s}.idba_ud/split/none/PROFILE.db'\n",
    "    contigsDB = f'{assem_dir}/{s}.idba_ud/split/none/CONTIGS.db'\n",
    "    out = f'{assem_dir}/{s}.idba_ud/split/none'\n",
    "\n",
    "    cococt_input_cmd = f'anvi-export-splits-and-coverages -p {profileDB} -c {contigsDB} -o {out} -O {s} --splits-mode'\n",
    "    concoct_input.append(cococt_input_cmd)\n",
    "    \n",
    "    run_concoct_cmd = f'sbatch --wrap \"concoct --coverage_file {out}/{s}-COVs.txt --composition_file {out}/{s}-SPLITS.fa -b {out}/{s}_concoct -r 150 -t 48 -c {c}\"' \n",
    "    run_concoct.append(run_concoct_cmd)\n",
    "    \n",
    "    csv_to_tsv = f'sed \"s/,/\\\\tbin_/g\" {out}/{s}_concoct_clustering_gt1000.csv > {out}/{s}_concoct_clustering_gt1000.tsv'\n",
    "    import_collection = f'anvi-import-collection {out}/{s}_concoct_clustering_gt1000.tsv -p {profileDB} -c {contigsDB} -C concoct'\n",
    "    \n",
    "    import_concoct.append(f'{csv_to_tsv}; {import_collection}')\n",
    "    \n",
    "# add to dataframe\n",
    "\n",
    "df['concoct_input'] = concoct_input\n",
    "df['run_concoct'] = run_concoct\n",
    "df['import_concoct'] = import_concoct\n",
    "\n",
    "# save files\n",
    "df[~df.sample_id.str.contains('CTRL')]['concoct_input'].to_csv(f'{outdir}/anvio_concoct_input.sh', index=False, quoting=csv.QUOTE_NONE, header=False)\n",
    "df[~df.sample_id.str.contains('CTRL')]['run_concoct'].to_csv(f'{outdir}/anvio_run_concoct.sh', index=False, quoting=csv.QUOTE_NONE, header=False)\n",
    "df[~df.sample_id.str.contains('CTRL')]['import_concoct'].to_csv(f'{outdir}/anvio_import_concoct.sh', index=False, quoting=csv.QUOTE_NONE, sep='\\t', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ba8a3e21-6702-4354-81f6-e9d03952ca76",
   "metadata": {},
   "outputs": [],
   "source": [
    "anvio = 'conda run -n anvio-7.1'\n",
    "outfile = f'{outdir}/anvio_run_kegg_split.sh'\n",
    "with open(outfile, 'w') as f:\n",
    "    for row in df.itertuples():\n",
    "        s = row.sample_id\n",
    "        contigsDB = f'{assem_dir}/{s}.idba_ud/split/none/CONTIGS.db'\n",
    "        f.write(f'sbatch --wrap \"{anvio} anvi-run-kegg-kofams -c {contigsDB} -T 48\"\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c74665-b6e9-4c98-8d8f-d958ede398c4",
   "metadata": {},
   "source": [
    "# mapping and Anvio profiling for dereplicated refined MAGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "668bf0c2-d54d-4264-96c4-1ee7574b6bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping to dereplicated refined RO MAGs\n",
    "\n",
    "shrinksam = '/shared/software/bin/shrinksam'\n",
    "bt2 = '/shared/software/bin/bowtie2'\n",
    "anvio = 'conda run -n anvio-dev'\n",
    "\n",
    "mapping_df = []\n",
    "\n",
    "drep_dir = '/groups/banfield/projects/industrial/nelson_lab/awtp2/dRep_RO2_BULK'\n",
    "scaffolds = '/groups/banfield/projects/industrial/nelson_lab/awtp2/dRep_RO2_BULK/RO_drep_refined.fasta'\n",
    "bt2base = '/groups/banfield/projects/industrial/nelson_lab/awtp2/dRep_RO2_BULK/bt2/RO_drep_refined.fasta'\n",
    "contigsDB = '/groups/banfield/projects/industrial/nelson_lab/awtp2/dRep_RO2_BULK/RO_mags.db'\n",
    "s = 'dRep_RO2_BULK'\n",
    "\n",
    "for row in df.itertuples():\n",
    "    r = row.sample_id\n",
    "    r_fixed = re.sub('[\\.-]', '_', r) # if your sample name has dashes or periods in it, Anvio won't like it.\n",
    "    r1 = f'{reads_dir}/{r}_trim_clean.PE.1.fastq.gz'\n",
    "    r2 = f'{reads_dir}/{r}_trim_clean.PE.2.fastq.gz'        \n",
    "    raw_bam = f'{drep_dir}/{s}-vs-{r}-raw.bam'\n",
    "    filtered_bam_raw = f'{drep_dir}/{s}-vs-{r}-raw-filt.bam'\n",
    "    bam = f'{drep_dir}/{s}-vs-{r}.bam'\n",
    "\n",
    "    if 'CTRL' in r or 'CONTROL' in r:\n",
    "        continue\n",
    "    \n",
    "    # filter mapping to 5 mismatches (removed shrinksam so anvio can tell how many reads mapped)\n",
    "    map_cmd = f'sbatch --wrap \"{bt2} -p 48 -x {bt2base} -1 {r1} -2 {r2} | sambam > {raw_bam}; ' \\\n",
    "              f'reformat.sh in={raw_bam} out={filtered_bam_raw} editfilter=5 threads=48; rm {raw_bam}\"'\n",
    "\n",
    "    # process mapping\n",
    "    initbam = f'anvi-init-bam {filtered_bam_raw} -o {bam}; rm {filtered_bam_raw}'\n",
    "\n",
    "    # anvi-profile\n",
    "    profile_out = f'{drep_dir}/anvio_data/{r}_profile' # check this\n",
    "    anvip_cmd = f'sbatch --wrap \"{anvio} anvi-profile --min-contig-length 1000 --skip-SNV-profiling -T 48 -i {bam} -c {contigsDB} -o {profile_out} -S {r_fixed}\"'\n",
    "\n",
    "    ## append all commands to table\n",
    "    mapping_df.append([r, map_cmd, initbam, anvip_cmd])\n",
    "mapping_df = pd.DataFrame.from_records(mapping_df, columns=['reads', 'map', 'initbam', 'anvi-profile'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8388edd-0ec4-432e-8246-3e449347e902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping to dereplicated refined RO and MF MAGs\n",
    "\n",
    "shrinksam = '/shared/software/bin/shrinksam'\n",
    "bt2 = '/shared/software/bin/bowtie2'\n",
    "anvio = 'conda run -n anvio-dev'\n",
    "\n",
    "mapping_df = []\n",
    "\n",
    "drep_dir = '/groups/banfield/projects/industrial/nelson_lab/awtp2/dRep_membranes'\n",
    "scaffolds = '/groups/banfield/projects/industrial/nelson_lab/awtp2/dRep_membranes/contigs.fasta'\n",
    "bt2base = '/groups/banfield/projects/industrial/nelson_lab/awtp2/dRep_membranes/bt2/contigs.fasta'\n",
    "contigsDB = '/groups/banfield/projects/industrial/nelson_lab/awtp2/dRep_membranes/contigs.db'\n",
    "s = 'dRep_membranes'\n",
    "\n",
    "for row in df.itertuples():\n",
    "    r = row.sample_id\n",
    "    r_fixed = re.sub('[\\.-]', '_', r) # if your sample name has dashes or periods in it, Anvio won't like it.\n",
    "    r1 = f'{reads_dir}/{r}_trim_clean.PE.1.fastq.gz'\n",
    "    r2 = f'{reads_dir}/{r}_trim_clean.PE.2.fastq.gz'        \n",
    "    raw_bam = f'{drep_dir}/{s}-vs-{r}-raw.bam'\n",
    "    filtered_bam_raw = f'{drep_dir}/{s}-vs-{r}-raw-filt.bam'\n",
    "    bam = f'{drep_dir}/{s}-vs-{r}.bam'\n",
    "\n",
    "    if 'CTRL' in r or 'CONTROL' in r:\n",
    "        continue\n",
    "    \n",
    "    # filter mapping to 5 mismatches (removed shrinksam so anvio can tell how many reads mapped)\n",
    "    map_cmd = f'sbatch --wrap \"{bt2} -p 48 -x {bt2base} -1 {r1} -2 {r2} | sambam > {raw_bam}; ' \\\n",
    "              f'reformat.sh in={raw_bam} out={filtered_bam_raw} editfilter=5 threads=48; rm {raw_bam}\"'\n",
    "\n",
    "    # process mapping\n",
    "    initbam = f'anvi-init-bam {filtered_bam_raw} -o {bam}; rm {filtered_bam_raw}'\n",
    "\n",
    "    # anvi-profile\n",
    "    profile_out = f'{drep_dir}/anvio_data/{r}_profile' # check this\n",
    "    # note: this time, doing SNV profiling, so removed --skip-SNV-profiling\n",
    "    anvip_cmd = f'sbatch --wrap \"{anvio} anvi-profile --min-contig-length 1000 -T 48 -i {bam} -c {contigsDB} -o {profile_out} -S {r_fixed}\"'\n",
    "\n",
    "    ## append all commands to table\n",
    "    mapping_df.append([r, map_cmd, initbam, anvip_cmd])\n",
    "mapping_df = pd.DataFrame.from_records(mapping_df, columns=['reads', 'map', 'initbam', 'anvi-profile'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d31b8306-c247-4e2a-8565-fbdbd6c34e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_df['map'].to_csv(f'{outdir}/map_to_drep_membranes.sh', index=False, quoting=csv.QUOTE_NONE, header=False)\n",
    "mapping_df['initbam'].to_csv(f'{outdir}/initbam_drep_membranes.sh', index=False, quoting=csv.QUOTE_NONE, header=False)\n",
    "mapping_df['anvi-profile'].to_csv(f'{outdir}/profile_drep_membranes.sh', index=False, quoting=csv.QUOTE_NONE, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effa3cb7-406d-4633-8bd5-f2c969f61d20",
   "metadata": {},
   "source": [
    "# Running InStrain on the mappings to dereplicated membrane MAGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf369e80-7a82-496d-8b1a-3e3165773631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on one genome (only ran on 2 samples for this test but generated commands for all)\n",
    "drep_dir = '/groups/banfield/projects/industrial/nelson_lab/awtp2/dRep_membranes'\n",
    "scaffolds = '/groups/banfield/projects/industrial/nelson_lab/awtp2/dRep_membranes/contigs.fasta'\n",
    "scaffold2bin = '/groups/banfield/projects/industrial/nelson_lab/awtp2/dRep_membranes/scaffold_to_bin.tsv'\n",
    "contigs = '/groups/banfield/projects/industrial/nelson_lab/awtp2/dRep_membranes/contigs.fasta'\n",
    "\n",
    "# for test run, using --scaffolds_to_profile to include just the aquabacterium\n",
    "# note that the bam files generated above were already filtered to allow only 5 mismatches per read, consider rerunning\n",
    "\n",
    "with open(f'{outdir}/instrain_aquabacterium3test.sh', \"w\") as f:\n",
    "    for row in df.itertuples():        \n",
    "        r = row.sample_id\n",
    "        r_fixed = re.sub('[\\.-]', '_', r)\n",
    "        outprefix = f'{drep_dir}/instrain/{r_fixed}'\n",
    "        \n",
    "        # only profile bins with mappings of the MF and RO samples\n",
    "        if 'RO2' in r or 'MFCOMB' in r:        \n",
    "            bam = f\"{drep_dir}/dRep_membranes-vs-{r_fixed}.bam\"\n",
    "            cmd = f'sbatch --wrap \"inStrain profile -o {outprefix} -p 48 -s {scaffold2bin} --scaffolds_to_profile {drep_dir}/aquabacterium_3_scaffolds.txt {bam} {contigs}\"'\n",
    "            f.write(cmd+\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "11e5420b-2580-490b-a159-92a2eed316ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on one genome (only ran on 2 samples for this test but generated commands for all)\n",
    "drep_dir = '/groups/banfield/projects/industrial/nelson_lab/awtp2/dRep_membranes'\n",
    "scaffolds = '/groups/banfield/projects/industrial/nelson_lab/awtp2/dRep_membranes/contigs.fasta'\n",
    "scaffold2bin = '/groups/banfield/projects/industrial/nelson_lab/awtp2/dRep_membranes/scaffold_to_bin.tsv'\n",
    "contigs = '/groups/banfield/projects/industrial/nelson_lab/awtp2/dRep_membranes/contigs.fasta'\n",
    "\n",
    "# for test run, using --scaffolds_to_profile to include just the aquabacterium\n",
    "# note that the bam files generated above were already filtered to allow only 5 mismatches per read, consider rerunning\n",
    "\n",
    "with open(f'{outdir}/instrain_drep_membrane_genomes.sh', \"w\") as f:\n",
    "    for row in df.itertuples():        \n",
    "        r = row.sample_id\n",
    "        r_fixed = re.sub('[\\.-]', '_', r)\n",
    "        outprefix = f'{drep_dir}/instrain/{r_fixed}'\n",
    "        \n",
    "        # drop controls\n",
    "        if 'CTRL' in r or 'CONTROL' in r:\n",
    "            continue\n",
    "        \n",
    "        # only profile bins with mappings of the MF and RO samples\n",
    "        if 'RO2' in r or 'MFCOMB' in r:        \n",
    "            bam = f\"{drep_dir}/dRep_membranes-vs-{r_fixed}.bam\"\n",
    "            cmd = f'sbatch --wrap \"inStrain profile -o {outprefix} -p 48 -s {scaffold2bin} \\\n",
    "--min_scaffold_reads 10  --min_genome_coverage 1 --min_read_ani 0.96 \\\n",
    "{bam} {contigs}\"'\n",
    "            f.write(cmd+\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae82a06f-56bd-4eb8-83c3-c4d9834d1362",
   "metadata": {},
   "source": [
    "# Pull 16S, 18S, SCG for taxonomy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6551ac27-28db-4d79-9def-4ebec688ec3e",
   "metadata": {
    "tags": []
   },
   "source": [
    "- who is present and how abundant? across how many samples?\n",
    "- do we have bins for them?\n",
    "- does this match to the 16S V4 amplicon data\n",
    "\n",
    "1. pull 16S and 18S\n",
    "2. cluster at 100% across samples\n",
    "3. classify with SILVA's on line SINA classifier\n",
    "4. compile into table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41254601-fc90-452b-a310-8b879b629fe2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# must migrate contigs.dbs to 7.1 and run this with 7.1\n",
    "anvi_pull_18S_cmd = []\n",
    "anvi_pull_16S_cmd = []\n",
    "\n",
    "for row in df.itertuples():\n",
    "    s = row.sample_id\n",
    "    \n",
    "    contigsDB = f'{assem_dir}/{s}.idba_ud/split/none/CONTIGS.db'\n",
    "    profileDB = f'{assem_dir}/{s}.idba_ud/anvio_data/{s}_profile/PROFILE.db'\n",
    "    \n",
    "    out = f'{assem_dir}/{s}.idba_ud/split/none/18S.fasta'\n",
    "    anvi_pull_18S = f'anvi-get-sequences-for-hmm-hits -c {contigsDB} --hmm-sources Ribosomal_RNA_18S -o {out}'\n",
    "    anvi_pull_18S_cmd.append(anvi_pull_18S)\n",
    "    \n",
    "    out = f'{assem_dir}/{s}.idba_ud/split/none/16S.fasta'\n",
    "    anvi_pull_16S = f'anvi-get-sequences-for-hmm-hits -c {contigsDB} --hmm-sources Ribosomal_RNA_16S -o {out}'\n",
    "    anvi_pull_16S_cmd.append(anvi_pull_16S)\n",
    "\n",
    "df['anvi_pull_18S_cmd'] = anvi_pull_18S_cmd\n",
    "df['anvi_pull_18S_cmd'].to_csv(f'{outdir}/anvio_pull_18S_cmd.sh', index=False, quoting=csv.QUOTE_NONE, header=False, sep='\\t')\n",
    "\n",
    "df['anvi_pull_16S_cmd'] = anvi_pull_16S_cmd\n",
    "df['anvi_pull_16S_cmd'].to_csv(f'{outdir}/anvio_pull_16S_cmd.sh', index=False, quoting=csv.QUOTE_NONE, header=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d423cc44-c16e-4ebb-a47c-ab427b2835b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# estimate SCG taxonomy on decontaminated data\n",
    "anvio = 'conda run -n anvio-dev'\n",
    "\n",
    "outfile = f'{outdir}/anvio_runscg_split.sh'\n",
    "with open(outfile, 'w') as f:\n",
    "\n",
    "    for row in df[~df.sample_id.str.contains('CTRL')].itertuples():\n",
    "        s = row.sample_id\n",
    "        contigsDB = f'{assem_dir}/{s}.idba_ud/split/none/CONTIGS.db'\n",
    "        profileDB = f'{assem_dir}/{s}.idba_ud/split/none/PROFILE.db'\n",
    "        anvrunscg = f'sbatch --wrap \"{anvio} anvi-run-scg-taxonomy -c {contigsDB} -T 48\"'\n",
    "        f.write(anvrunscg+'\\n')\n",
    "\n",
    "outfile = f'{outdir}/anvio_estscg_split.sh'\n",
    "with open(outfile, 'w') as f:\n",
    "\n",
    "    for row in df[~df.sample_id.str.contains('CTRL')].itertuples():\n",
    "        s = row.sample_id\n",
    "        contigsDB = f'{assem_dir}/{s}.idba_ud/split/none/CONTIGS.db'\n",
    "        profileDB = f'{assem_dir}/{s}.idba_ud/split/none/PROFILE.db'\n",
    "        out = f'{assem_dir}/{s}.idba_ud/split/none/{s}_scg_taxonomy.txt'\n",
    "        anvestscg = f'sbatch --wrap \"{anvio} anvi-estimate-scg-taxonomy -c {contigsDB} -T 48 --metagenome-mode -o {out} -p {profileDB} --compute-scg-coverages -p {profileDB} --compute-scg-coverages -S Ribosomal_S3_C\"'\n",
    "        f.write(anvestscg+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2e0090-6506-4707-b6ab-408b12e056e7",
   "metadata": {},
   "source": [
    "# Read mapping to human virus genomes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6dde5f-1bb3-46ca-ac34-090004278da3",
   "metadata": {},
   "source": [
    "clustered polyomaviruses and adenoviruses at 99% ID before mapping so that we can threshold coverage depth and hopefully avoid having coverage split across multiple genomes...\n",
    "\n",
    "usearch -sortbylength polyomavirus_adenovirus.fasta -fastaout polyomavirus_adenovirus.sorted.fasta\n",
    "\n",
    "usearch -cluster_fast polyomavirus_adenovirus.sorted.fasta -id 0.99 -uc polyomavirus_adenovirus.uc -centroids polyomavirus_adenovirus.centroids.fasta -threads 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c65a6d4-6efd-4ab1-bf1f-c123acc18701",
   "metadata": {},
   "outputs": [],
   "source": [
    "bt2 = '/shared/software/bin/bowtie2'\n",
    "virus_dir = '/groups/banfield/projects/industrial/nelson_lab/awtp2/virus_analysis'\n",
    "\n",
    "virus_mapping_df = []\n",
    "\n",
    "outfile = f'{outdir}/virus_map.sh'\n",
    "with open(outfile, 'w') as f:\n",
    "    \n",
    "    for row in df.itertuples():  \n",
    "        s = row.sample_id\n",
    "\n",
    "        if 'CTRL' in s or 'CONTROL' in s:\n",
    "            continue\n",
    "\n",
    "        bt2base = f'{virus_dir}/bt2/polyomavirus_adenovirus.centroids.fasta'\n",
    "        r1 = f'{reads_dir}/{s}_trim_clean.PE.1.fastq.gz'\n",
    "        r2 = f'{reads_dir}/{s}_trim_clean.PE.2.fastq.gz'        \n",
    "        raw_bam = f'{virus_dir}/{s}-vs-polyomavirus_adenovirus_centroids-raw.bam'\n",
    "\n",
    "        map_cmd = f'sbatch --wrap \"{bt2} -p 48 -x {bt2base} -1 {r1} -2 {r2} --no-unal | sambam > {raw_bam}\"'\n",
    "        \n",
    "        f.write(map_cmd+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "849ec342-cd9c-483e-9bb0-d711b7d3771a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort and index virus mappings\n",
    "virus_dir = '/groups/banfield/projects/industrial/nelson_lab/awtp2/virus_analysis'\n",
    "\n",
    "outfile = f'{outdir}/virus_count.sh'\n",
    "with open(outfile, 'w') as f:\n",
    "    \n",
    "    for row in df.itertuples():  \n",
    "        s = row.sample_id\n",
    "\n",
    "        if 'CTRL' in s or 'CONTROL' in s:\n",
    "            continue\n",
    "        raw_bam = f'{virus_dir}/{s}-vs-polyomavirus_adenovirus_centroids-raw.bam'\n",
    "        filtered_bam = f'{virus_dir}/{s}-vs-polyomavirus_adenovirus_centroids.bam'\n",
    "        sorted_bam = f'{virus_dir}/{s}-vs-polyomavirus_adenovirus_centroids.sorted.bam'\n",
    "\n",
    "        # calculate coverage breadth at >=1x depth for each target, ignore reads shorter than 75 bp\n",
    "        # then say it's present, in which case, count\n",
    "        samtools_cmds = f'samtools sort {raw_bam} > {sorted_bam}; '\\\n",
    "                        f'samtools index {sorted_bam}; '\\\n",
    "                        f'samtools idxstats {sorted_bam} > {virus_dir}/{s}-vs-polyomavirus_adenovirus_centroids.stats.txt; '\\\n",
    "                        f'samtools coverage --min-read-len 75 {sorted_bam} > {virus_dir}/{s}-vs-polyomavirus_adenovirus_centroids.cov.txt'\n",
    "\n",
    "        f.write(samtools_cmds+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb59e1d-c54e-4ee2-88d9-65ebd9a5bd48",
   "metadata": {},
   "source": [
    "# ARG analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac40558-5c55-4c27-84e3-84ddeadee087",
   "metadata": {},
   "source": [
    "Loaded CARD db including wildcard, according to this guide: https://github.com/arpcard/rgi/blob/master/docs/rgi_bwt.rst\n",
    "\n",
    "Steps\n",
    "1. filter to remove reads shorter than 75 bp (bbduk)\n",
    "2. Count how many reads remain in each sample (seqkit)\n",
    "3. subsample filtered reads to even depth (seqtk)\n",
    "4. run rgi bwt analysis to map to CARD and wildCARD using kma mapping and parse output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1c5938d4-3511-4f14-beea-611b0261fdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter reads\n",
    "reads_dir = '/groups/banfield/projects/industrial/nelson_lab/awtp2/reads/trimmed'\n",
    "filtered_dir = '/groups/banfield/projects/industrial/nelson_lab/awtp2/reads/filtered'\n",
    "\n",
    "outfile = f'{outdir}/filter_reads75_forARGmapping.sh'\n",
    "with open(outfile, 'w') as f:\n",
    "\n",
    "    for row in df.itertuples():\n",
    "        s = row.sample_id\n",
    "        \n",
    "        if 'CTRL' in s or 'CONTROL' in s:\n",
    "            continue\n",
    "            \n",
    "        r1 = f'{reads_dir}/{s}_trim_clean.PE.1.fastq.gz'\n",
    "        r2 = f'{reads_dir}/{s}_trim_clean.PE.2.fastq.gz'\n",
    "        out1 = f'{filtered_dir}/{s}_filtered.PE.1.fastq.gz'\n",
    "        out2 = f'{filtered_dir}/{s}_filtered.PE.2.fastq.gz'\n",
    "\n",
    "        cmd = f'sbatch --wrap \"bbduk.sh in1={r1} in2={r2} out1={out1} out2={out2} qtrim=r trimq=10 minlen=75 threads=48\"'\n",
    "        \n",
    "        f.write(cmd+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351110cf-959a-4721-888b-6f5c3af58ba5",
   "metadata": {},
   "source": [
    "from within the filtered reads dir, run:\n",
    "\n",
    "`seqkit stat *.fastq.gz > seqkit_output.tsv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fd44f917-33b4-429f-93e6-1b9372622b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter reads\n",
    "filtered_dir = '/groups/banfield/projects/industrial/nelson_lab/awtp2/reads/filtered'\n",
    "subsampled = '/groups/banfield/projects/industrial/nelson_lab/awtp2/reads/subsampled'\n",
    "\n",
    "outfile = f'{outdir}/subsample_reads_forARGmapping.sh'\n",
    "with open(outfile, 'w') as f:\n",
    "\n",
    "    for row in df.itertuples():\n",
    "        s = row.sample_id\n",
    "        \n",
    "        if 'CTRL' in s or 'CONTROL' in s:\n",
    "            continue\n",
    "\n",
    "        r1 = f'{filtered_dir}/{s}_filtered.PE.1.fastq.gz'\n",
    "        r2 = f'{filtered_dir}/{s}_filtered.PE.2.fastq.gz'\n",
    "        sub1 = f'{subsampled}/{s}_sub.PE.1.fastq.gz'\n",
    "        sub2 = f'{subsampled}/{s}_sub.PE.2.fastq.gz'\n",
    "\n",
    "        cmd = f'seqtk sample -s100 {r1} 9000000 > {sub1}; seqtk sample -s100 {r2} 9000000 > {sub2}'\n",
    "        \n",
    "        f.write(cmd+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf597fc-b4f0-4ca7-925b-0f76da01d57c",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "\n",
    "Annotation:\n",
    "- KEGG HMMs\n",
    "- Pfam HMMs (if desired)\n",
    "- estimate metabolism (curious about this)\n",
    "\n",
    "Additional questions:\n",
    "- annotate with special HMMs (e.g. ARGs, METABOLIC)\n",
    "- investigate metabolisms of key organisms of interest\n",
    "- Virsorter2 to look for phage\n",
    "- look for pathogens\n",
    "- phylogenetic trees for key organisms of interest\n",
    "- assess growth via iRep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1632c405-2770-4cad-be74-a90a0ecfda8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
